{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LogisticRegression, BayesianRidge\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import log_loss, roc_auc_score, precision_score, recall_score, accuracy_score, \\\n",
    "    f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataImputer():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cat_estimator = None\n",
    "        self.num_estimator = None\n",
    "        self.input_df = None\n",
    "        self.imputed_df = None\n",
    "        self.cat_imp = None\n",
    "        self.num_imp = None \n",
    "        self.hot_flag = False\n",
    "        self.feature_map = dict()\n",
    "\n",
    "    def set_state(self, cat_estimator = None, num_estimator = None, input_df = None):\n",
    "        self.cat_estimator = cat_estimator\n",
    "        self.num_estimator = num_estimator\n",
    "        self.input_df = input_df\n",
    "\n",
    "    def get_state(self):\n",
    "        return (self.imputed_df, self.num_imp, self.cat_imp)\n",
    "\n",
    "    def map_features(self):\n",
    "\n",
    "        for iColumn in self.input_df.columns:\n",
    "            if self.input_df[iColumn].dtypes != float:\n",
    "                indx = self.input_df.index[self.input_df[iColumn].isnull().values == True]\n",
    "                self.feature_map[iColumn] = indx\n",
    "\n",
    "    def impute_data(self):\n",
    "\n",
    "\n",
    "        if self.hot_flag:\n",
    "            num_imp = self.num_imp\n",
    "            cat_imp = self.cat_imp\n",
    "        else:\n",
    "            num_imp = IterativeImputer(estimator=self.num_estimator, max_iter=1000, tol=1e-3, skip_complete=True)\n",
    "            cat_imp = IterativeImputer(estimator=self.cat_estimator, max_iter=1000, tol=1e-3, skip_complete=True)\n",
    "\n",
    "        # Fit numerical first\n",
    "        if self.hot_flag:\n",
    "            imputed_data = num_imp.transform(self.input_df)\n",
    "        else:\n",
    "            imputed_data = num_imp.fit_transform(self.input_df)\n",
    "\n",
    "        imputed_df = pd.DataFrame(data=imputed_data, columns=self.input_df.columns)\n",
    "\n",
    "        # Refill nan in categorical \n",
    "        for iColumn in self.feature_map:\n",
    "            indx = self.feature_map[iColumn]\n",
    "            imputed_df.loc[indx,iColumn] = np.nan\n",
    "\n",
    "        # Impute the categorical\n",
    "        if self.hot_flag:\n",
    "            imputed_data = cat_imp.transform(imputed_df)\n",
    "        else:\n",
    "            imputed_data = cat_imp.fit_transform(imputed_df)\n",
    "        imputed_df = pd.DataFrame(data=imputed_data, columns=self.input_df.columns)\n",
    "\n",
    "        if self.hot_flag: \n",
    "            pass\n",
    "        else:\n",
    "        # Store the results\n",
    "            self.num_imp = num_imp\n",
    "            self.cat_imp = cat_imp\n",
    "            self.hot_flag = True\n",
    "        \n",
    "        self.imputed_df = imputed_df\n",
    "\n",
    "def StrBinarize(input_df=pd.DataFrame, target_col='', target_label=''):\n",
    "    if input_df.empty:\n",
    "        print('Non empty Pandas dataframe must be passed!')\n",
    "    else:\n",
    "        input_df[target_col+'_encode'] = 1\n",
    "        indx = input_df.index[input_df[target_col]==target_label].tolist()\n",
    "        input_df.loc[indx,target_col+'_encode'] = 0 \n",
    "    \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcROCCurve(y_test, y_score):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    for i in range(np.shape(y_score)[1]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test, y_score[:,i])\n",
    "\n",
    "    return (fpr, tpr)\n",
    "\n",
    "def LogROCCurve(fpr, tpr):\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    \n",
    "\n",
    "    fig = plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(\n",
    "        fpr[1],\n",
    "        tpr[1],\n",
    "        color=\"darkorange\",\n",
    "        lw=lw\n",
    "    )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic example\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('output/roc_curve.png')\n",
    "\n",
    "def LogMetrics(model,X_test, y_test):\n",
    "\n",
    "    ypred = model.predict(X_test)\n",
    "    y_scores = model.predict_proba(X_test)    \n",
    "\n",
    "    fpr, tpr = CalcROCCurve(y_test,y_scores)\n",
    "    LogROCCurve(fpr, tpr)\n",
    "\n",
    "    crDict = classification_report(y_test, ypred, labels=[0, 1], target_names=['Not Readmitted', 'Readmitted'],output_dict=True)\n",
    "    LogDict = {'':list(), 'precision':list(), 'recall':list(),'f1-score':list(), 'support':list()}\n",
    "    for iRow in crDict.keys():\n",
    "        if iRow == 'accuracy':\n",
    "            continue\n",
    "        else:\n",
    "            LogDict[''].append(iRow)\n",
    "            for iCol in crDict[iRow]:\n",
    "                LogDict[iCol].append(crDict[iRow][iCol])\n",
    "\n",
    "    MetricDict = dict()\n",
    "    MetricDict['Accuracy'] = accuracy_score(y_test, ypred)\n",
    "    MetricDict['F1 Score'] = f1_score(y_test, ypred)\n",
    "    MetricDict['AUC'] = roc_auc_score(y_test, y_scores[:,1])\n",
    "    MetricDict['Precision'] = precision_score(y_test, ypred)\n",
    "    MetricDict['Recall'] = recall_score(y_test, ypred)\n",
    "    MetricDict['logloss'] = log_loss(y_test, ypred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, ypred).ravel()\n",
    "    ppv = tp / (tp+fp)\n",
    "    npv = tn / (tn+fn)\n",
    "    MetricDict['PPV'] = ppv\n",
    "    MetricDict['NPV'] = npv\n",
    "    \n",
    "    cm = confusion_matrix(y_test, ypred).ravel()\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\n",
    "    \n",
    "    MetricDict['True Negative'] = tn\n",
    "    MetricDict['False Positive'] = fp\n",
    "    MetricDict['False Negative'] = fn\n",
    "    MetricDict['True Positive'] = tp\n",
    "    \n",
    "    for iKey in MetricDict:\n",
    "        print(iKey+':', MetricDict[iKey])\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildSamplingDict(HyperParameterDict):\n",
    "    paramDict = dict()\n",
    "    for iKey in HyperParameterDict:\n",
    "            sampleType = HyperParameterDict[iKey][0]\n",
    "            #x0 = HyperParameterDict[iKey][1]\n",
    "            #x1 = HyperParameterDict[iKey][2]\n",
    "\n",
    "            if sampleType == 'Real':\n",
    "                paramDict[iKey] = Real(HyperParameterDict[iKey][1],HyperParameterDict[iKey][2])\n",
    "            elif sampleType == 'Categorical':\n",
    "                paramDict[iKey] = Categorical(HyperParameterDict[iKey][1])\n",
    "            elif sampleType == 'Integer':\n",
    "                paramDict[iKey] = Integer(HyperParameterDict[iKey][1],HyperParameterDict[iKey][2])\n",
    "            else:\n",
    "                print(f'Unspecified sampling {sampleType} type given for {iKey}')\n",
    "                print('Skipping hyperparameter. Check your input!')\n",
    "\n",
    "    return paramDict\n",
    "\n",
    "def TuneHyperParameters(\n",
    "        X_train=None, y_train=None,\n",
    "        estimator=None, param_dict={}, metric='roc_auc', n_iter=20):\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "    \n",
    "    cv_i = list(kf.split(X_train, y_train))\n",
    "    \n",
    "    opt = BayesSearchCV(\n",
    "        estimator,\n",
    "        param_dict,\n",
    "        scoring=metric,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv_i,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    opt.fit(X_train, y_train,callback=DeltaYStopper(delta=1e-2,n_best=5))\n",
    "\n",
    "    return opt\n",
    "\n",
    "def EvaluateModel(X_test=None, y_test=None,\n",
    "                  estimator=None):\n",
    "    \n",
    "    return estimator.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
